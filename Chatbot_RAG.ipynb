{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import gradio as gr\n",
    "import requests\n",
    "import time\n",
    "from collections import deque\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayush\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:161: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ayush\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:161: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ayush\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:161: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.storage import StorageContext\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core.prompts.prompts import SimpleInputPrompt\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prevent CUDA fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CareerGuidanceRAG:\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        llm_model_name=\"stabilityai/stablelm-zephyr-3b\",\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        self.device = device\n",
    "\n",
    "        # Define query wrapper prompt\n",
    "        query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "        # Initialize embedding model\n",
    "        self.embedding_model = HuggingFaceEmbedding(\n",
    "            model_name=embedding_model_name,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # Initialize LLM with 4-bit quantization\n",
    "        try:\n",
    "            self.llm = HuggingFaceLLM(\n",
    "                model_name=llm_model_name,\n",
    "                tokenizer_name=llm_model_name,\n",
    "                context_window=4096,\n",
    "                max_new_tokens=512,\n",
    "                generate_kwargs={\"temperature\": 0.8, \"do_sample\": True},\n",
    "                system_prompt=\"\"\"\n",
    "                   You are a Q&A assistant. Your goal is to answer questions as\n",
    "                   accurately as possible based on the instructions and context provided.\n",
    "                   \"\"\",\n",
    "                query_wrapper_prompt=query_wrapper_prompt,\n",
    "                device_map=\"auto\",\n",
    "                model_kwargs={\n",
    "                    \"torch_dtype\": torch.float16,\n",
    "                    \"load_in_4bit\": True\n",
    "                }\n",
    "            )\n",
    "        except RuntimeError:\n",
    "            print(\"CUDA OOM! Falling back to CPU...\")\n",
    "            self.device = \"cpu\"\n",
    "            self.llm = HuggingFaceLLM(\n",
    "                model_name=llm_model_name,\n",
    "                tokenizer_name=llm_model_name,\n",
    "                context_window=4096,\n",
    "                max_new_tokens=512,\n",
    "                generate_kwargs={\"temperature\": 0.8, \"do_sample\": True},\n",
    "                system_prompt=\"\"\"\n",
    "                You are a Q&A assistant. Your goal is to answer questions as\n",
    "                accurately as possible based on the instructions and context provided.\n",
    "                \"\"\",\n",
    "                query_wrapper_prompt=query_wrapper_prompt,\n",
    "                device_map=\"auto\",\n",
    "                model_kwargs={\n",
    "                    \"torch_dtype\": torch.float16,\n",
    "                    \"load_in_4bit\": True\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Set up LlamaIndex settings\n",
    "        Settings.chunk_size = 1024\n",
    "        Settings.llm = self.llm\n",
    "        Settings.embed_model = self.embedding_model\n",
    "\n",
    "        self.storage_context = StorageContext.from_defaults()\n",
    "        self.index = None\n",
    "\n",
    "    def load_documents_from_folder(self, folder_path: str):\n",
    "        print(f\"Loading documents from {folder_path}...\")\n",
    "        folder_path = Path(folder_path)\n",
    "\n",
    "        documents = SimpleDirectoryReader(\n",
    "            input_dir=str(folder_path),\n",
    "            recursive=True,\n",
    "            exclude_hidden=True\n",
    "        ).load_data()\n",
    "\n",
    "        self.index = VectorStoreIndex.from_documents(\n",
    "            documents,\n",
    "            storage_context=self.storage_context,\n",
    "            show_progress=True\n",
    "        )\n",
    "\n",
    "    def generate(self, query: str) -> str:\n",
    "        if self.index is None:\n",
    "            return \"Document index not initialized. Please upload documents first.\"\n",
    "\n",
    "        query_engine = self.index.as_query_engine(\n",
    "            similarity_top_k=5,\n",
    "            # response_mode=\"tree_summarize\"\n",
    "        )\n",
    "\n",
    "        return str(query_engine.query(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayush\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from data/scraped...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38633ed5a93d44b293693c4b64c7bb49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c725040eae6418eb977ed000c40d245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b92b4c7f1f24422ad88ba665f21fc9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "rag = CareerGuidanceRAG()\n",
    "rag.load_documents_from_folder(\"data/scraped\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after salary cleaning: 28989\n",
      "Unique experience levels found: Entry, Mid, Senior\n",
      "\n",
      "===== Model Evaluation Metrics =====\n",
      "\n",
      "Minimum :\n",
      "R¬≤ Score: 0.8781\n",
      "Mean Absolute Error: ‚Çπ32770.56\n",
      "Root Mean Squared Error: ‚Çπ177869.51\n",
      "Explained Variance Score: 0.8781\n",
      "\n",
      "Maximum :\n",
      "R¬≤ Score: 0.9991\n",
      "Mean Absolute Error: ‚Çπ62411.51\n",
      "Root Mean Squared Error: ‚Çπ279618.96\n",
      "Explained Variance Score: 0.9991\n",
      "Prediction error plot saved to model_evaluation_results/min_salary_prediction_20250413_151850.png\n",
      "Prediction error plot saved to model_evaluation_results/max_salary_prediction_20250413_151850.png\n",
      "Feature importance plot saved to model_evaluation_results/feature_importance_20250413_151850.png\n",
      "Precision matrix plot saved to model_evaluation_results/precision_matrix_20250413_151850.png\n",
      "Confusion matrices plot saved to model_evaluation_results/confusion_matrices_20250413_151850.png\n",
      "\n",
      "Metrics saved to model_evaluation_results/model_metrics_20250413_151850.csv\n",
      "Salary prediction model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import salary utility functions\n",
    "from salary_utils import (\n",
    "    years_to_experience_level, extract_job_role_and_experience,\n",
    "    is_daily_salary_query, is_monthly_salary_query, is_salary_query\n",
    ")\n",
    "try: # Try to import the salary predictor model\n",
    "    from nlp_salary_predictor import NLPSalaryPredictor, parse_user_input\n",
    "    # Initialize the model\n",
    "    salary_model = NLPSalaryPredictor()\n",
    "    has_salary_model = True\n",
    "    print(\"Salary prediction model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load salary prediction model: {e}\")\n",
    "    has_salary_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4038 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "c:\\Users\\ayush\\anaconda3\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "custom_css = \"\"\"\n",
    "/* Modern Gradient-based Color Scheme */\n",
    ":root {\n",
    "    --primary: #7C4DFF;    /* Deep purple */\n",
    "    --secondary: #2A2E45;  /* Navy blue */\n",
    "    --background: #1A1C28; /* Space black */\n",
    "    --accent: #FF6B6B;     /* Coral pink */\n",
    "    --text: #F0F0FF;       /* Soft lavender */\n",
    "}\n",
    "\n",
    "/* Base Styling with Larger Fonts */\n",
    "body {\n",
    "    font-size: 19px !important;\n",
    "    line-height: 1.7 !important;\n",
    "    background: linear-gradient(135deg, var(--background) 0%, #242736 100%);\n",
    "    font-family: 'Inter', system-ui, sans-serif;\n",
    "    color: var(--text) !important;\n",
    "}\n",
    "\n",
    "/* Text Elements Scaling */\n",
    "h1 { font-size: 2.8rem !important; }\n",
    "h2 { font-size: 2.2rem !important; }\n",
    "h3 { font-size: 1.9rem !important; }\n",
    "p { font-size: 1.1em !important; }\n",
    "\n",
    "/* Chat Interface */\n",
    ".gr-box {\n",
    "    background: rgba(42, 46, 69, 0.85) !important;\n",
    "    backdrop-filter: blur(12px);\n",
    "    border-radius: 18px;\n",
    "    border: 1px solid rgba(124, 77, 255, 0.25);\n",
    "    font-size: 1.15rem !important;\n",
    "}\n",
    "\n",
    "/* Reduced gap between buttons */\n",
    ".button-row {\n",
    "    gap: 5px !important;\n",
    "}\n",
    "\n",
    "/* Conversation text spacing */\n",
    ".gr-textbox span {\n",
    "    line-height: 1.4 !important;\n",
    "    margin-bottom: 0.3rem !important;\n",
    "}\n",
    "\n",
    "/* Chat display customization */\n",
    "#chat-display {\n",
    "    padding: 8px !important;\n",
    "    margin-bottom: 10px !important;\n",
    "}\n",
    "\n",
    "#chat-display textarea {\n",
    "    line-height: 1.3 !important;\n",
    "}\n",
    "\n",
    "/* Input Fields */\n",
    "input[type=\"text\"], textarea {\n",
    "    font-size: 1.2rem !important;\n",
    "    padding: 18px !important;\n",
    "    background: rgba(255, 255, 255, 0.08) !important;\n",
    "    border: 2px solid rgba(124, 77, 255, 0.3) !important;\n",
    "}\n",
    "\n",
    "/* Buttons */\n",
    "button {\n",
    "    font-size: 1.25rem !important;\n",
    "    padding: 16px 28px !important;\n",
    "    background-image: linear-gradient(135deg, var(--primary) 0%, #906BFF 100%) !important;\n",
    "    border-radius: 14px !important;\n",
    "}\n",
    "\n",
    "button:hover {\n",
    "    transform: translateY(-2px) scale(1.02);\n",
    "    box-shadow: 0 10px 20px rgba(124, 77, 255, 0.25) !important;\n",
    "}\n",
    "\n",
    "/* Job Listings */\n",
    ".job-card {\n",
    "    background: linear-gradient(145deg, #2A2E45 0%, #1F2235 100%);\n",
    "    border-radius: 16px;\n",
    "    padding: 24px;\n",
    "    font-size: 1.1rem !important;\n",
    "}\n",
    "\n",
    ".job-card h4 {\n",
    "    font-size: 1.5rem !important;\n",
    "    background: linear-gradient(45deg, var(--primary), var(--accent));\n",
    "    -webkit-background-clip: text;\n",
    "    -webkit-text-fill-color: transparent;\n",
    "}\n",
    "\n",
    "/* Chat History */\n",
    "#chat-history {\n",
    "    font-size: 1.2rem !important;\n",
    "    line-height: 1.8 !important;\n",
    "    padding: 24px !important;\n",
    "}\n",
    "\n",
    "/* Interactive Elements */\n",
    ".gr-textbox:focus, button:focus {\n",
    "    box-shadow: 0 0 0 4px rgba(124, 77, 255, 0.3) !important;\n",
    "}\n",
    "\n",
    "/* Scrollbar */\n",
    "::-webkit-scrollbar {\n",
    "    width: 10px;\n",
    "    background: var(--secondary);\n",
    "}\n",
    "\n",
    "::-webkit-scrollbar-thumb {\n",
    "    background: linear-gradient(var(--primary), var(--accent));\n",
    "    border-radius: 6px;\n",
    "}\n",
    "\n",
    "/* Section Dividers */\n",
    ".section-divider {\n",
    "    height: 4px;\n",
    "    background: linear-gradient(90deg, transparent 0%, var(--primary) 50%, transparent 100%);\n",
    "    margin: 2.5rem 0;\n",
    "}\n",
    "\n",
    "/* Status Messages */\n",
    ".gr-label {\n",
    "    font-size: 1.3rem !important;\n",
    "    letter-spacing: 0.5px;\n",
    "}\n",
    "\n",
    "/* Responsive Scaling */\n",
    "@media (max-width: 768px) {\n",
    "    body { font-size: 17px !important; }\n",
    "    h1 { font-size: 2.2rem !important; }\n",
    "    button { font-size: 1.1rem !important; }\n",
    "}\n",
    "\"\"\" \n",
    "\n",
    "# Multi-Turn Memory\n",
    "conversation_history = deque(maxlen=5)\n",
    "\n",
    "# ===== CHATBOT FUNCTIONS =====\n",
    "def chatbot(input_text):\n",
    "    \"\"\"Handles chatbot queries with multi-turn memory.\"\"\"\n",
    "    conversation_history.append(f\"üë§ User: {input_text}\")\n",
    "    \n",
    "    # Check if this is a salary-related query\n",
    "    if is_salary_query(input_text):\n",
    "        try:\n",
    "            # If nlp_salary_predictor is available, use it\n",
    "            if has_salary_model:\n",
    "                job_role, experience_level = parse_user_input(input_text)\n",
    "            else:\n",
    "                # Use our internal implementation\n",
    "                job_role, experience_level = extract_job_role_and_experience(input_text)\n",
    "            \n",
    "            # Try to get a salary prediction\n",
    "            try:\n",
    "                if has_salary_model:\n",
    "                    prediction_result = salary_model.predict_salary(job_role, experience_level)\n",
    "                else:\n",
    "                    # Mock prediction if model isn't available\n",
    "                    prediction_result = {\n",
    "                        'job_role_matched': job_role,\n",
    "                        'experience_level_matched': experience_level,\n",
    "                        'min_salary': 500000 if \"entry\" in experience_level.lower() else (1000000 if \"mid\" in experience_level.lower() else 2000000),\n",
    "                        'max_salary': 1000000 if \"entry\" in experience_level.lower() else (2000000 if \"mid\" in experience_level.lower() else 3500000)\n",
    "                    }\n",
    "                \n",
    "                # Check if the user wants daily or monthly salary\n",
    "                is_daily = is_daily_salary_query(input_text)\n",
    "                is_monthly = is_monthly_salary_query(input_text)\n",
    "                \n",
    "                if is_daily:\n",
    "                    # Assume 260 working days per year (52 weeks √ó 5 days)\n",
    "                    min_salary = prediction_result['min_salary'] / 260\n",
    "                    max_salary = prediction_result['max_salary'] / 260\n",
    "                    period = \"daily\"\n",
    "                elif is_monthly:\n",
    "                    min_salary = prediction_result['min_salary'] / 12\n",
    "                    max_salary = prediction_result['max_salary'] / 12\n",
    "                    period = \"monthly\"\n",
    "                else:\n",
    "                    min_salary = prediction_result['min_salary']\n",
    "                    max_salary = prediction_result['max_salary']\n",
    "                    period = \"yearly\"\n",
    "                \n",
    "                # Format the response nicely\n",
    "                response = f\"Based on my analysis for {prediction_result['job_role_matched']} at {prediction_result['experience_level_matched']} level:\\n\"\n",
    "                response += f\"The {period} salary range is typically between ‚Çπ{min_salary:,.2f} and ‚Çπ{max_salary:,.2f}.\"\n",
    "                \n",
    "                # If we provided daily or monthly, also provide annual for reference\n",
    "                if is_daily or is_monthly:\n",
    "                    response += f\"\\nOn an annual basis, this would be ‚Çπ{prediction_result['min_salary']:,.2f} to ‚Çπ{prediction_result['max_salary']:,.2f}.\"\n",
    "            except Exception as e:\n",
    "                # If prediction fails, use query_engine\n",
    "                print(f\"Salary prediction failed: {e}, using query_engine\")\n",
    "                full_context = input_text\n",
    "                response = rag.generate(full_context)\n",
    "        except Exception as e:\n",
    "            # If parsing fails, use query_engine\n",
    "            print(f\"Salary parsing failed: {e}, using query_engine\")\n",
    "            full_context = input_text\n",
    "    else:\n",
    "        # For non-salary queries, use the general query engine\n",
    "        full_context = input_text\n",
    "        try:\n",
    "            return rag.generate(full_context)  # Ensure query_engine is initialized\n",
    "        except Exception as e:\n",
    "            # Fallback if query_engine is not available\n",
    "            print(f\"Query engine error: {e}\")\n",
    "            response = f\"You said: {full_context}\"\n",
    "\n",
    "    conversation_history.append(f\"\\nü§ñ Assistant: {response}\\n\")\n",
    "    \n",
    "    # Return formatted conversation with single newlines\n",
    "    return \"\\n\".join(conversation_history)\n",
    "\n",
    "def clear_chat():\n",
    "    \"\"\"Clears the conversation history.\"\"\"\n",
    "    conversation_history.clear()\n",
    "    return \"\"\n",
    "\n",
    "# ===== LINKEDIN JOB SEARCH FUNCTIONS =====\n",
    "def scrape_linkedin_jobs(query, location=\"\"):\n",
    "    \n",
    "    \n",
    "    \"\"\"Scrape LinkedIn job listings based on search query\"\"\"\n",
    "    base_url = \"https://www.linkedin.com/jobs/search/\"\n",
    "    params = {\n",
    "        \"keywords\": query,\n",
    "        \"location\": location,\n",
    "        \"position\": 1,\n",
    "        \"pageNum\": 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Configure headers to mimic browser behavior\n",
    "        HEADERS = {\n",
    "                 \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\", \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            }\n",
    "        response = requests.get(base_url, params=params, headers=HEADERS)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        jobs = []\n",
    "        job_listings = soup.find_all('div', class_='base-card')\n",
    "        \n",
    "        for job in job_listings[:10]:  # Limit to 10 results\n",
    "            title = job.find('h3', class_='base-search-card__title').text.strip()\n",
    "            company = job.find('a', class_='hidden-nested-link').text.strip()\n",
    "            job_location = job.find('span', class_='job-search-card__location').text.strip()\n",
    "            link = job.find('a', class_='base-card__full-link')['href']\n",
    "            \n",
    "            jobs.append({\n",
    "                'title': title,\n",
    "                'company': company,\n",
    "                'location': job_location,\n",
    "                'link': link.split('?')[0]  # Clean URL\n",
    "            })\n",
    "            \n",
    "            time.sleep(0.5)  # Be polite with requests\n",
    "            \n",
    "        return jobs\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def display_jobs(query, location):\n",
    "    \n",
    "    # Validate query to ensure it's job-related\n",
    "    if not is_valid_job_query(query):\n",
    "        return \"<div style='color: #FF6B6B; background-color: rgba(255, 107, 107, 0.1); padding: 15px; border-radius: 10px; border-left: 4px solid #FF6B6B;'><strong>‚ö†Ô∏è Invalid Job Query:</strong> Please enter a valid job title, role, or industry. Examples: 'Software Engineer', 'Data Scientist', 'Marketing', 'Healthcare'.</div>\"\n",
    "    \n",
    "    jobs = scrape_linkedin_jobs(query, location)\n",
    "    \n",
    "    if isinstance(jobs, str):  # Error case\n",
    "        return f\"<div style='color: red'>{jobs}</div>\"\n",
    "    \n",
    "    if not jobs:\n",
    "        return \"<div style='color: orange'>No jobs found for this search.</div>\"\n",
    "    \n",
    "    html_output = f\"<h3>Found {len(jobs)} jobs:</h3>\"\n",
    "    for idx, job in enumerate(jobs, 1):\n",
    "        html_output += f\"\"\"\n",
    "        <div style='margin-bottom: 20px; border-bottom: 1px solid #ccc; padding-bottom: 10px;'>\n",
    "            <h4>{idx}. {job['title']}</h4>\n",
    "            <p><strong>Company:</strong> {job['company']}</p>\n",
    "            <p><strong>Location:</strong> {job['location']}</p>\n",
    "            <p><a href=\"{job['link']}\" target=\"_blank\">View Job</a></p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    return html_output\n",
    "\n",
    "def is_valid_job_query(query):\n",
    "    \"\"\"\n",
    "    Validate if the query appears to be job-related.\n",
    "    Returns True for valid job queries, False otherwise.\n",
    "    \"\"\"\n",
    "    if not query or len(query.strip()) < 2:\n",
    "        return False\n",
    "        \n",
    "    # Common job-related terms/prefixes/suffixes\n",
    "    job_related_terms = [\n",
    "        \"developer\", \"engineer\", \"manager\", \"assistant\", \"specialist\", \"analyst\", \n",
    "        \"designer\", \"consultant\", \"coordinator\", \"director\", \"technician\", \"representative\",\n",
    "        \"administrator\", \"supervisor\", \"officer\", \"programmer\", \"scientist\", \"associate\",\n",
    "        \"intern\", \"job\", \"career\", \"position\", \"role\", \"hiring\", \"recruitment\",\n",
    "        \"full-time\", \"part-time\", \"remote\", \"work\", \"employment\",\n",
    "        # Industries\n",
    "        \"tech\", \"it\", \"software\", \"health\", \"medical\", \"finance\", \"banking\", \"education\",\n",
    "        \"retail\", \"sales\", \"marketing\", \"hr\", \"legal\", \"media\", \"design\", \"construction\",\n",
    "        \"manufacturing\", \"engineering\", \"science\", \"research\", \"data\", \"ai\", \"ml\",\n",
    "        # Roles\n",
    "        \"ceo\", \"cto\", \"cfo\", \"vp\", \"head\", \"lead\", \"junior\", \"senior\", \"mid\", \"staff\"\n",
    "    ]\n",
    "    \n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    # Check if any job-related term is in the query\n",
    "    for term in job_related_terms:\n",
    "        if term in query_lower or query_lower in term:\n",
    "            return True\n",
    "            \n",
    "    # If query is very long, likely not a job search\n",
    "    if len(query_lower) > 50:\n",
    "        return False\n",
    "        \n",
    "    # If query contains question marks or specific non-job phrases\n",
    "    if \"?\" in query or \"who is\" in query_lower or \"what is\" in query_lower or \"how to\" in query_lower:\n",
    "        return False\n",
    "        \n",
    "    # Default to allowing the query if we're not sure\n",
    "    return True\n",
    "\n",
    "\n",
    "# Create the combined Gradio interface\n",
    "with gr.Blocks(css=custom_css, title=\"AI Assistant with Job Search\") as demo:\n",
    "    gr.Markdown(\"# ü§ñ **Career Guidance AI Chatbot **\", elem_id=\"title\")\n",
    "    \n",
    "    # Chatbot Section (Top)\n",
    "    with gr.Group():\n",
    "        gr.Markdown(\"### üí¨ Chat with an intelligent assistant!\", elem_id=\"subtitle\")\n",
    "        \n",
    "        chatbox = gr.Textbox(label=\"Conversation History\", interactive=False, lines=12, elem_id=\"chat-display\")\n",
    "        user_input = gr.Textbox(lines=1, placeholder=\"Type your message...\", label=\"Your Message\")\n",
    "        \n",
    "        with gr.Row(elem_classes=[\"button-row\"]):\n",
    "            submit_btn = gr.Button(\"üöÄ Send\")\n",
    "            clear_btn = gr.Button(\"üóëÔ∏è Clear Chat\")\n",
    "\n",
    "        submit_btn.click(fn=chatbot, inputs=user_input, outputs=chatbox)\n",
    "        user_input.submit(fn=chatbot, inputs=user_input, outputs=chatbox)\n",
    "        clear_btn.click(fn=clear_chat, outputs=chatbox)\n",
    "    \n",
    "    # Divider\n",
    "    gr.Markdown(\"---\", elem_id=\"divider\", elem_classes=[\"section-divider\"])\n",
    "    \n",
    "    # LinkedIn Job Search Section (Bottom)\n",
    "    with gr.Group():\n",
    "        gr.Markdown(\"## üîç LinkedIn Job Search\", elem_id=\"subtitle\")\n",
    "        gr.Markdown(\"Enter your job search query below:\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            job_query = gr.Textbox(label=\"Job title or keywords\", placeholder=\"Software Engineer\")\n",
    "            job_location = gr.Textbox(label=\"Location (optional)\", placeholder=\"New York\")\n",
    "        \n",
    "        search_btn = gr.Button(\"üîç Search Jobs\")\n",
    "        \n",
    "        job_output = gr.HTML()\n",
    "        \n",
    "        search_btn.click(\n",
    "            fn=display_jobs,\n",
    "            inputs=[job_query, job_location],\n",
    "            outputs=job_output,\n",
    "        )\n",
    "\n",
    "# Define prediction function for Salary Predictor UI\n",
    "def predict_salary_from_query(query):\n",
    "    if not query or query.strip() == \"\":\n",
    "        return \"Please enter a job role or query.\"\n",
    "    \n",
    "    try:\n",
    "        # Parse natural language input to extract job role and experience\n",
    "        if has_salary_model:\n",
    "            job_role, experience_level = parse_user_input(query)\n",
    "        else:\n",
    "            job_role, experience_level = extract_job_role_and_experience(query)\n",
    "        \n",
    "        parsed_info = f\"## Parsed Input\\n\"\n",
    "        parsed_info += f\"- Job Role: **{job_role}**\\n\"\n",
    "        parsed_info += f\"- Experience Level: **{experience_level if experience_level else 'Not specified (using Entry-Level)'}**\\n\\n\"\n",
    "        \n",
    "        # Get prediction\n",
    "        if has_salary_model:\n",
    "            result = salary_model.predict_salary(job_role, experience_level)\n",
    "        else:\n",
    "            # Mock prediction if model isn't available\n",
    "            result = {\n",
    "                'job_role_matched': job_role,\n",
    "                'experience_level_matched': experience_level,\n",
    "                'min_salary': 500000 if \"entry\" in str(experience_level).lower() else (1000000 if \"mid\" in str(experience_level).lower() else 2000000),\n",
    "                'max_salary': 1000000 if \"entry\" in str(experience_level).lower() else (2000000 if \"mid\" in str(experience_level).lower() else 3500000)\n",
    "            }\n",
    "        \n",
    "        if isinstance(result, dict):\n",
    "            # Check if the user wants specific salary period\n",
    "            is_monthly = is_monthly_salary_query(query)\n",
    "            is_daily = is_daily_salary_query(query)\n",
    "            \n",
    "            # Create detailed output\n",
    "            output = parsed_info\n",
    "            output += f\"## Matched To\\n\"\n",
    "            output += f\"- Job Role: **{result['job_role_matched']}**\\n\"\n",
    "            output += f\"- Experience Level: **{result['experience_level_matched']}**\\n\\n\"\n",
    "            \n",
    "            # Display salary in all formats\n",
    "            output += f\"## Annual Salary Range\\n\"\n",
    "            output += f\"- Annual Minimum: **‚Çπ{result['min_salary']:,.2f}**\\n\"\n",
    "            output += f\"- Annual Maximum: **‚Çπ{result['max_salary']:,.2f}**\\n\\n\"\n",
    "            \n",
    "            output += f\"## Monthly Salary Range\\n\"\n",
    "            output += f\"- Monthly Minimum: **‚Çπ{result['min_salary']/12:,.2f}**\\n\"\n",
    "            output += f\"- Monthly Maximum: **‚Çπ{result['max_salary']/12:,.2f}**\\n\\n\"\n",
    "            \n",
    "            # Assume 260 working days per year (52 weeks √ó 5 days)\n",
    "            output += f\"## Daily Salary Range\\n\"\n",
    "            output += f\"- Daily Minimum: **‚Çπ{result['min_salary']/260:,.2f}**\\n\"\n",
    "            output += f\"- Daily Maximum: **‚Çπ{result['max_salary']/260:,.2f}**\\n\"\n",
    "            \n",
    "            return output\n",
    "        else:\n",
    "            return parsed_info + str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error processing your query: {str(e)}\"\n",
    "    \n",
    "# Custom CSS for Styling\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
